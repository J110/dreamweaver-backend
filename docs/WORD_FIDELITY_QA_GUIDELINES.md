# Dream Valley — Word Fidelity QA Guidelines

> **Purpose**: Ensure every generated audio file accurately narrates the source text. This document explains the Voxtral-based QA pipeline, how fidelity scoring works, how to interpret results, and how to fix failures.

---

## 1. What Is Word Fidelity QA?

Word Fidelity QA verifies that the audio narration generated by Chatterbox TTS matches the original written text. It catches:

| Issue | Description | How It's Caught |
|-------|-------------|-----------------|
| **Hallucination** | TTS speaks entirely different text | Low fidelity score (<0.49) |
| **Skipping** | TTS drops words, sentences, or paragraphs | Low word order score |
| **Repetition** | TTS repeats phrases not in source | Transcript longer than expected + low overlap |
| **Garbling** | TTS produces unintelligible speech | Low character ratio + low word coverage |
| **Wrong language** | TTS switches languages mid-narration | Transcript contains unexpected script |

### Why It Matters

Even when TTS audio "sounds" fine to a casual listen, it may have subtle issues:
- A paragraph was replaced with similar-sounding but different text (hallucination)
- A few sentences in the middle were skipped (shortening the story)
- Names or places were mispronounced so badly they became different words

Automated QA catches these issues systematically across hundreds of files.

---

## 2. The QA Pipeline — Three Phases

### Phase 1: Duration Anomaly Detection (Free, Instant)

**What it does**: Compares the duration of each voice variant against the median for that story. Flags any voice deviating >15% from median.

**Why**: If `female_2` is 2 minutes but all other voices are ~3 minutes for the same story, `female_2` likely skipped content.

**Cost**: Free — uses duration metadata from content.json, no API calls.

**Thresholds**:
- Deviation <= 15%: Normal (voices naturally vary in speed)
- Deviation > 15%: WARN — investigate manually
- Deviation > 25%: Likely content issue — listen and verify

**Known exceptions**: `female_1` voice naturally speaks ~15-20% slower than others. Consistent slow deviation across all stories for one voice is a voice characteristic, not a quality issue.

### Phase 2: Voxtral Transcription + Text Fidelity (Primary QA)

**What it does**:
1. Sends each MP3 file to Voxtral Mini (Mistral's speech-to-text model)
2. Gets back a text transcription of what was spoken
3. Compares the transcription against the original source text
4. Produces a fidelity score (0.0–1.0)

**Cost**: ~$0.003 per minute of audio (Voxtral transcription pricing).

**This is the core of word fidelity QA.**

### Phase 3: Voxtral Quality Scoring (Optional, Deeper Analysis)

**What it does**: Sends audio + source text to Voxtral's chat mode for holistic quality evaluation (pronunciation, fluency, pacing, emotion delivery, noise).

**Cost**: Token-based (Voxtral chat pricing).

**When to use**: Run on WARN/FAIL files from Phase 2, or on all files for comprehensive QA (`--full-qa`).

---

## 3. Text Fidelity Scoring — How It Works

### 3.1 Text Preprocessing

Before comparison, both source text and transcript are normalized:

**For all languages:**
- Strip emotion markers (`[GENTLE]`, `[PAUSE]`, etc.) from source text
- Remove all punctuation (periods, commas, quotes, dashes)
- Collapse whitespace (newlines, tabs, multiple spaces -> single space)
- Lowercase all Latin characters

**Additional for Hindi/Devanagari:**
- Remove nukta (U+093C): ड़→ड, ज़→ज, फ़→फ
- Normalize chandrabindu → anusvara: ँ→ं
- Remove visarga (ः) and avagraha (ऽ)

### 3.2 Four Metrics

The fidelity check computes four complementary metrics:

#### Metric 1: Fuzzy Word Coverage (Weight: 50%)

The **primary metric**. Walks through each word in the source text in order and tries to find a fuzzy match in the transcript:

```
Source:     "Pina pressed her nose against the window"
Transcript: "Pina pressed her nose against the window"
Coverage:   7/7 = 1.00 (perfect)

Source:     "Pina pressed her nose against the window"
Transcript: "Pina pressed against the window"
Coverage:   5/7 = 0.71 (skipped "her nose")
```

**How fuzzy matching works**:
- Uses `difflib.SequenceMatcher` to compute character-level similarity between each source word and candidate transcript words
- A match is accepted if similarity >= 0.5 (e.g., "lighthouse" vs "lighthaus" = 0.82, accepted)
- Maintains word order — once a transcript word is consumed, it can't match again
- Uses a lookahead window of 8 words to handle minor reorderings

**Why this is the primary metric**: It handles the natural TTS→ASR pipeline noise (slight pronunciation differences that cause transcription variations) while still catching skipped or hallucinated content.

#### Metric 2: Word Order Score (Weight: 25%)

Longest Common Subsequence (LCS) of exact word matches, divided by the max length. This catches:
- Content spoken in wrong order
- Large blocks of text moved or rearranged
- Significant rewording while keeping some same words

#### Metric 3: Character Ratio (Weight: 15%)

`difflib.SequenceMatcher` character-level similarity between the full normalized texts. This provides a holistic "how similar do these texts look?" score.

#### Metric 4: Word Overlap (Weight: 10%)

Jaccard similarity of word sets (intersection / union). This catches:
- Completely different vocabulary (hallucination)
- Very different content even if some words match

### 3.3 Combined Score

```
combined = 0.50 * fuzzy_word_coverage
         + 0.25 * word_order_score
         + 0.15 * character_ratio
         + 0.10 * word_overlap
```

### 3.4 Verdicts

| Verdict | Combined Score | Meaning |
|---------|---------------|---------|
| **PASS** | >= 0.70 | Audio faithfully narrates the source text |
| **WARN** | 0.49 – 0.69 | Partial match — some content may be missing or altered |
| **FAIL** | < 0.49 | Audio significantly deviates from source text |

---

## 4. Running QA

### Basic Commands

```bash
# Full 3-phase QA on English content
python3 scripts/qa_audio.py --lang en

# Duration check only (free, instant)
python3 scripts/qa_audio.py --lang en --duration-only

# Transcription fidelity only (no quality scoring)
python3 scripts/qa_audio.py --lang en --no-quality-score

# Single story
python3 scripts/qa_audio.py --story-id gen-586908c26fc2

# Single voice across all stories
python3 scripts/qa_audio.py --voice female_1

# Custom fidelity threshold
python3 scripts/qa_audio.py --lang en --threshold 0.80

# Dry run (show plan, estimate cost)
python3 scripts/qa_audio.py --dry-run
```

### Recommended Workflow

1. **After any audio generation**: Run `--no-quality-score` (Phase 1+2) as minimum
2. **For release candidates**: Run full 3-phase QA
3. **For quick checks**: Run `--duration-only` (free, instant)
4. **For specific issues**: Run with `--story-id` or `--voice` filters

---

## 5. Interpreting QA Reports

### Report Location

Reports are saved to: `seed_output/qa_reports/qa_audio_YYYYMMDD_HHMMSS.json`

A symlink `qa_audio_latest.json` always points to the most recent report.

### Report Structure

```json
{
  "summary": {
    "total_variants": 14,
    "passed": 12,
    "warned": 2,
    "failed": 0,
    "avg_fidelity": 0.92
  },
  "stories": [
    {
      "story_id": "gen-586908c26fc2",
      "title": "Pina and the Whispering Lighthouse",
      "variants": [
        {
          "voice": "female_1",
          "duration_seconds": 180.5,
          "text_fidelity": {
            "fuzzy_word_coverage": 0.95,
            "ratio": 0.91,
            "word_overlap": 0.88,
            "word_order_score": 0.93,
            "combined": 0.93
          },
          "verdict": "PASS"
        }
      ]
    }
  ]
}
```

### Reading the Summary

| Field | Meaning |
|-------|---------|
| `total_variants` | Total audio files checked |
| `passed` | Files that faithfully match source text |
| `warned` | Files with partial match — review recommended |
| `failed` | Files that significantly deviate — must be regenerated |
| `avg_fidelity` | Average combined score across all files |

### What "Good" Looks Like

| Metric | Good | Acceptable | Bad |
|--------|------|-----------|-----|
| avg_fidelity | > 0.90 | 0.75–0.90 | < 0.75 |
| passed % | > 90% | 75–90% | < 75% |
| failed count | 0 | 1–2 | > 2 |

---

## 6. Transcript Caching

### How It Works

Voxtral transcriptions are cached in `seed_output/qa_reports/transcript_cache.json`, keyed by audio filename (e.g., `gen-5869_female_1.mp3`).

### Benefits

- **Cost savings**: Re-running QA doesn't re-transcribe unchanged files
- **Speed**: Cached transcripts are loaded instantly vs. 5-15s per API call
- **Idempotency**: Same file always produces same comparison result

### When to Invalidate Cache

Clear or regenerate cache entries when:
- Audio file has been regenerated (same filename, new content)
- Source text has been edited (fidelity will be recalculated against new text)
- QA pipeline code has changed significantly

```bash
# Delete the cache to force full re-transcription
rm seed_output/qa_reports/transcript_cache.json

# Or selectively delete entries for regenerated files
python3 -c "
import json
cache = json.load(open('seed_output/qa_reports/transcript_cache.json'))
# Remove entries for regenerated story
cache = {k: v for k, v in cache.items() if not k.startswith('gen-5869')}
json.dump(cache, open('seed_output/qa_reports/transcript_cache.json', 'w'), indent=2)
"
```

---

## 7. Common QA Patterns and What They Mean

### Pattern 1: All voices PASS for a story
**Meaning**: Content is solid. No action needed.

### Pattern 2: One voice WARN, rest PASS
**Meaning**: Usually a voice-specific issue (e.g., female_1 speaks slower, duration outlier). Listen to the WARN variant manually. If it sounds fine, accept it.

### Pattern 3: One voice FAIL, rest PASS
**Meaning**: That specific voice had a TTS generation issue (hallucination or skip on that particular run). Regenerate just that variant:
```bash
python3 scripts/generate_audio.py --story-id <id> --voice <voice> --force --speed 0.8
```

### Pattern 4: All voices WARN for a story (similar scores)
**Meaning**: Likely a source text issue rather than TTS issue. The text may have unusual vocabulary, proper nouns, or formatting that confuses both TTS and ASR. Check:
- Are there unusual words the ASR might transcribe differently?
- Is the text very short (under 50 words)?
- Does the text have lots of dialogue with quotes?

### Pattern 5: All voices FAIL for a story
**Meaning**: Critical issue. Either:
- Source text is in wrong language for the TTS language setting
- `annotated_text` field is malformed or empty
- Audio files are corrupted or empty
- Text is extremely long without paragraph breaks (hallucination)

### Pattern 6: Duration outlier but PASS on fidelity
**Meaning**: The voice speaks at a different pace but narrates all the correct content. Usually acceptable — just a voice characteristic.

---

## 8. Fixing Failures

### Step 1: Identify the Cause

```bash
# Check the QA report
python3 -c "
import json
report = json.load(open('seed_output/qa_reports/qa_audio_latest.json'))
for story in report.get('stories', []):
    for v in story.get('variants', []):
        if v.get('verdict') in ('FAIL', 'WARN'):
            print(f\"{story['title']} / {v['voice']}: {v['verdict']} (combined={v['text_fidelity']['combined']:.2f})\")
"
```

### Step 2: Diagnose

| Combined Score | Fuzzy Coverage | Likely Cause | Fix |
|---------------|----------------|--------------|-----|
| < 0.30 | < 0.30 | Complete hallucination | Check text length, add chunking, regenerate |
| 0.30–0.50 | 0.40–0.60 | Partial hallucination / major skipping | Check paragraph breaks, chunk long segments |
| 0.50–0.70 | 0.60–0.80 | Minor skipping or ASR noise | Regenerate; if persists, edit source text |
| 0.70+ but WARN | 0.80+ | Duration outlier only | Usually acceptable, listen manually |

### Step 3: Fix Source Text (if needed)

Common source text fixes:
- Add `\n\n` paragraph breaks (every 150-200 words)
- Keep paragraphs under 300 characters after marker removal
- Spell out numbers ("three" not "3") for ages 0-8
- Replace em-dashes (—) with commas or periods
- Avoid excessive ellipsis (...) — use `[PAUSE]` instead
- Extend very short segments (<20 chars) to avoid TTS hallucination

### Step 4: Regenerate

```bash
# Regenerate a single failed variant
python3 scripts/generate_audio.py --story-id <id> --voice <voice> --force --speed 0.8

# Regenerate all variants for a story
python3 scripts/generate_audio.py --story-id <id> --force --speed 0.8
```

### Step 5: Re-QA

```bash
# Clear cache for regenerated files, then re-run QA
python3 scripts/qa_audio.py --story-id <id> --lang en --no-quality-score
```

---

## 9. Thresholds Reference

### Fidelity Thresholds

| Threshold | Value | Configurable Via |
|-----------|-------|-----------------|
| PASS | >= 0.70 | `--threshold` flag |
| WARN | 0.49 – 0.69 | Hardcoded (FAIL threshold) |
| FAIL | < 0.49 | Hardcoded |

### Duration Outlier Threshold

| Threshold | Value | Notes |
|-----------|-------|-------|
| Outlier | > 15% deviation from story median | Flags WARN in Phase 1 |

### Fuzzy Word Match Threshold

| Parameter | Value | Notes |
|-----------|-------|-------|
| Word similarity | >= 0.5 | SequenceMatcher ratio between source word and transcript word |
| Lookahead | 8 words | How far ahead in transcript to search for a match |

---

## 10. Cost Estimation

| Operation | Cost | Time |
|-----------|------|------|
| Phase 1 (duration) | Free | Instant |
| Phase 2 (transcription) | ~$0.003/min audio | ~5-15s per file (API) |
| Phase 3 (quality scoring) | Token-based | ~10-20s per file |
| Full QA: 14 files (~2 stories) | ~$0.10 | ~3-5 min |
| Full QA: 140 files (~20 stories) | ~$0.50 | ~30-60 min |

**Note**: Transcript caching means re-runs are nearly free and instant for unchanged files.

---

## 11. Integration with Generation Workflow

### Recommended Integration

```
1. Generate content (story/poem text)
2. Add to content.json
3. Generate audio: python3 scripts/generate_audio.py --speed 0.8
4. Run QA: python3 scripts/qa_audio.py --lang en --no-quality-score   ← MANDATORY
5. Review report: check for FAIL/WARN
6. Fix + regenerate any failures
7. Re-QA regenerated files
8. Copy to frontend: cp audio/pre-gen/*.mp3 ../dreamweaver-web/public/audio/pre-gen/
9. Update seedData.js if needed
10. Deploy
```

**Rule**: Never deploy audio without running at least Phase 1+2 QA. Audio that sounds fine may have subtle content issues only detectable through transcription comparison.

---

## 12. Limitations

### What QA Can't Catch

- **Pronunciation quality**: A word may be correctly transcribed but poorly pronounced (Phase 3 helps here)
- **Emotional delivery**: Correct text but wrong tone (e.g., excited when it should be sleepy)
- **Background noise**: Subtle artifacts that don't affect transcription
- **Pacing issues**: Correct content but rushed or too slow delivery

### ASR-Specific Limitations

- **Proper nouns**: Unusual names may be transcribed differently (e.g., "Pina" → "Pena")
- **Homophones**: Words that sound alike but differ in spelling
- **Onomatopoeia**: "Shh", "Hush" may not be transcribed accurately
- **Singing/humming**: `[SINGING]` and `[HUMMING]` sections may produce poor transcriptions

### Mitigation

For proper noun sensitivity, consider:
- Adding the character name to a QA allowlist
- Using Phase 3 (quality scoring) for stories with unusual names
- Manual listening for any WARN with proper noun mismatches
